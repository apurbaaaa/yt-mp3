{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apurbaaaa/yt-mp3/blob/main/Fine_Tune_MMS_on_Common_Voice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBSYoWbi-45k"
      },
      "source": [
        "# **Fine-tuning MMS Adapter Models for Multi-Lingual ASR**\n",
        "\n",
        "***New (06/2023)***: *This blog post is strongly inspired by \"Fine-tuning XLS-R on Multi-Lingual ASR\" https://huggingface.co/blog/fine-tune-xlsr-wav2vec2* and can be seen as an improved version of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7YOT2mnUiea"
      },
      "source": [
        "**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by *Alexei Baevski, Michael Auli, and Alex Conneau*.  Soon after the superior performance of Wav2Vec2 was demonstrated on one of the most popular English datasets for ASR, called [LibriSpeech](https://huggingface.co/datasets/librispeech_asr), *Facebook AI* presented a multi-lingual version of Wav2Vec2, called [XLSR](https://arxiv.org/abs/2006.13979) and [XLS-R](https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/). XLSR stands for *cross-lingual speech representations* and refers to model's ability to learn speech representations that are useful across multiple languages.\n",
        "\n",
        "MetaAI's most recent release, the [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1000 spoken languages can be identified, transcribed and generated with the [ASR, LID and TTS checkpoints that were released](https://huggingface.co/models?other=mms).\n",
        "\n",
        "In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning.\n",
        "\n",
        "For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [\"Fine-tuning XLS-R on Multi-Lingual ASR\"](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).\n",
        "\n",
        "In our experiments, MMS' Adapter training is both more memory efficient, more robust and yields better performance for low-resource languages. For medium to high resource languages it can still be advantegous to fine-tune the whole checkpoint instead of using Adapter layers though.\n",
        "\n",
        "![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/mms_map.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preserving the world's language diversity**\n",
        "\n",
        "According to https://www.ethnologue.com/ around 3000, or 40% of all \"living\" languages, are endangered due to fewer and fewer native speakers.\n",
        "This trend will only continue in an increasingly globalized world.\n",
        "\n",
        "**MMS** is capable of transcribing many languages which are endangered, such as *Ari* or *Kaivi*. In the future, MMS can play a vital role in keeping languages alive by helping the remaining speakers to create written records and communicating in their native tongue.\n",
        "\n",
        "To adapt to 1000+ different vocabularies, **MMS** uses of Adapters - a training method where only a small fraction of model weights are trained.\n",
        "\n",
        "Adapter layers act like linguistic bridges, enabling the model to leverage knowledge from one language when deciphering another.\n",
        "\n",
        "## **Fine-tuning MMS**\n",
        "\n",
        "**MMS** unsupervised checkpoints were pre-trained on more than **half a million** hour of audio in over **1400** languages ranging from 300 million to one billion parameters.\n",
        "\n",
        "You can find the pretrained-only checkpoints on the ðŸ¤— Hub:\n",
        "\n",
        "- [**`mms-300m`**](https://huggingface.co/facebook/mms-300m)\n",
        "- [**`mms-1b`**](https://huggingface.co/facebook/mms-1b)\n",
        "\n",
        "Similar to [BERT's masked language modeling objective](http://jalammar.github.io/illustrated-bert/), MMS learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network during self-supervised pre-training (*i.e.* diagram on the left below).\n",
        "\n",
        "For Automatic Speech Recognitino (ASR), the pretrained `MMS-1B` checkpoint was further fine-tuned in supervised fasion on 1000+ languages with a joint vocabulary output layer. As a final step, the joint vocabulary output layer was then thrown away and **only** ca. 2.5M adapter weights are trained on specific languages. The adapter weights hereby include small linear projection layers for each attention block as well as a language-specific vocabulary output layer.\n",
        "\n",
        "**MMS**'s released three checkpoints fine-tuned for speech recognition (ASR) that have 102, 1107, and 1162 adapter weights respectively (one for each language):\n",
        "\n",
        "- [**`mms-1b-fl102`**](https://huggingface.co/facebook/mms-1b-fl102)\n",
        "- [**`mms-1b-l1107`**](https://huggingface.co/facebook/mms-1b-1107)\n",
        "- [**`mms-1b-all`**](https://huggingface.co/facebook/mms-1b-all)\n",
        "\n",
        "You can see that the base models are saved (as usual) as a [`model.safetensors` file](https://huggingface.co/facebook/mms-1b-all/blob/main/model.safetensors), but in addition these repositories have many adapter weights stored in the repository, *e.g.* under the name [`adapter.fra.safetensors`](https://huggingface.co/facebook/mms-1b-all/blob/main/adapter.fra.safetensors) for French.\n",
        "\n",
        "The Hugging Face docs explain very well how such checkpoints can be used for inference [here](https://huggingface.co/docs/transformers/main/en/model_doc/mms#loading), so in this blog post we will instead focus on learning how we can efficiently train highly performant adapter models based on any of the released ASR checkpoints."
      ],
      "metadata": {
        "id": "VWe4pIR8lRrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training adaptive weights\n",
        "\n",
        "In machine learning, [adapters](https://arxiv.org/pdf/1902.00751.pdf) are a method used to fine-tune pre-trained models while keeping the original model parameters unchanged. They do this by inserting small, trainable modules, called adapter layers, between the pre-existing layers of the model, which then adapt the model to a specific task without requiring extensive retraining.\n",
        "\n",
        "Adapters have a long history in speech recognition and especially **speaker recognition**. In speaker recognition, adapters have been effectively used to tweak pre-existing models to recognize individual speaker idiosyncrasies, as highlighted in [Gales and Woodland's (1996)](https://www.isca-speech.org/archive_v0/archive_papers/icslp_1996/i96_1832.pdf) and [Miao et al.'s (2014)](https://www.cs.cmu.edu/~ymiao/pub/tasl_sat.pdf) work. This approach not only greatly reduces computational requirements compared to full model, but also allows for better and more flexible speaker-specific adjustments.\n",
        "\n",
        "The work done in **MMS** leverages this idea of adapters for speech recognition across different languages. Adapter weights a fine-tuned to grasp unique phonetic and grammatical traits of each target language. Thereby, MMS enables a single large base model (*e.g.*, the [**`mms-1b-all`**](https://huggingface.co/facebook/mms-1b-all) checkpoint) and 1000+ small adapter layers (2.5M weights each for **`mms-1b-all`**) to comprehend and transcribe multiple languages. This dramatically reduces the computational demand of developing distinct models for each language.\n",
        "\n",
        "Great! Now that we understood the motivation and theory, let's look into fine-tuning a couple adapter weights for **`mms-1b-all`** ðŸ”¥"
      ],
      "metadata": {
        "id": "miNvI1EdCbtf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT_QrfWtsxIz"
      },
      "source": [
        "## Notebook Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kruqixOYHaIo"
      },
      "source": [
        "As done previously in the [\"Fine-tuning XLS-R on Multi-Lingual ASR\"](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) blog post, we fine-tune the model on the low resource ASR dataset of [Common Voice](https://huggingface.co/datasets/common_voice) that contains only *ca.* 4h of validated training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx9OdDYrCtQ1"
      },
      "source": [
        "Just like Wav2Vec2 or XLS-R, MMS is fine-tuned using Connectionist Temporal Classification (CTC), which is an algorithm that is used to train neural networks for sequence-to-sequence problems, such as ASR and handwriting recognition.\n",
        "\n",
        "I highly recommend reading the well-written blog post [*Sequence Modeling with CTC (2017)*](https://distill.pub/2017/ctc/) by Awni Hannun."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcHuXIaWyHZU"
      },
      "source": [
        "First, let's try to get a good GPU in our colab! With Google Colab's free version it's sadly becoming much harder to get access to a good GPU. With Google Colab Pro, however, one should easily get either a V100 or P100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YELVqGxMxnbG",
        "outputId": "12c0b0f6-91d0-4b39-fa18-396ab61b67eb"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 28 08:55:49 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e335hPmdtASZ"
      },
      "source": [
        "Before we start, let's install `datasets` and `transformers`. Also, we need the `torchaudio` to load audio files and `jiwer` to evaluate our fine-tuned model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric ${}^1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets[audio]\n",
        "!pip install evaluate\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install jiwer\n",
        "!pip install accelerate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xxt_LwxDQlO"
      },
      "source": [
        "We strongly suggest to upload your training checkpoints directly to the [ðŸ¤— Hub](https://huggingface.co/) while training. The [ðŸ¤— Hub](https://huggingface.co/) has integrated version control so you can be sure that no model checkpoint is getting lost during training.\n",
        "\n",
        "To do so you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlMSH3T3EazV"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mW-C1Nt-j7k"
      },
      "source": [
        "## Prepare Data, Tokenizer, Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeBosnY9BH3e"
      },
      "source": [
        "ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text.\n",
        "\n",
        "In ðŸ¤— Transformers, the MMS model is thus accompanied by both a tokenizer, called [Wav2Vec2CTCTokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer), and a feature extractor, called [Wav2Vec2FeatureExtractor](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2featureextractor).\n",
        "\n",
        "Let's start by creating the tokenizer to decode the predicted output classes to the output transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXEWEJGQPqD"
      },
      "source": [
        "### Create `Wav2Vec2CTCTokenizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmMikuNEKl_"
      },
      "source": [
        "Fine-tuned MMS models, such as [**`mms-1b-all`**](https://huggingface.co/facebook/mms-1b-all) already have a [tokenizer](https://huggingface.co/facebook/mms-1b-all/blob/main/tokenizer_config.json) accompanying the model checkpoint. However since we want to fine-tune the model on specific low-resource data of a certain language, it is recommended to fully remove the tokenizer, and vocabulary output layer and simply create a new ones based on the training data itself.\n",
        "\n",
        "Remember that Wav2Vec2-like models fine-tuned on CTC transcribe an audio file with a single forward pass by first processing the audio input into a secquence of processed context representations and then using the final vocabulary output layer to classify each context representation to a character that reperesents the transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5oRE8XjIUH3"
      },
      "source": [
        "The output size of this layer corresponds to the number of tokens in the vocabulary, and therefore only on the labeled dataset used for fine-tuning. So in the first step, we will take a look at the chosen dataset of Common Voice and define a vocabulary based on the transcriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idBczw8mWzgt"
      },
      "source": [
        "For this notebook, we will use [Common Voice's 6.1 dataset](https://huggingface.co/datasets/mozilla-foundation/common_voice_6_1) for Turkish. Turkish corresponds to the language code `\"tr\"`.\n",
        "\n",
        "Now we can use ðŸ¤— Datasets' simple API to download the data. The dataset name is `\"mozilla-foundation/common_voice_6_1\"`, the configuration name corresponds to the language code, which is `\"tr\"` in our case.\n",
        "\n",
        "**Note**: Before being able to download the dataset, you have to access it by logging into your Hugging Face account, going on the [dataset repo page](https://huggingface.co/datasets/mozilla-foundation/common_voice_6_1) and clicking on:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot from 2023-06-19 12-19-11.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "oR6l4HSVKwMf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee4g9rpLxll"
      },
      "source": [
        "Common Voice has many different splits including `invalidated`, which refers to data that was not rated as \"clean enough\" to be considered useful. In this notebook, we will only make use of the splits `\"train\"`, `\"validation\"` and `\"test\"`.\n",
        "\n",
        "Because the Turkish dataset is so small, we will merge both the validation and training data into a training dataset and only use the test data for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMXcWFFgCXU"
      },
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "\n",
        "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", split=\"train+validation\", use_auth_token=True)\n",
        "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", split=\"test\", use_auth_token=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5y5N_HMANq"
      },
      "source": [
        "Many ASR datasets only provide the target text, `'sentence'` for each audio array `'audio'` and file `'path'`. Common Voice actually provides much more information about each audio file, such as the `'accent'`, etc. Keeping the notebook as general as possible, we only consider the transcribed text for fine-tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbyq6lDgQc2a"
      },
      "source": [
        "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
        "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9Hq4e4NDT9"
      },
      "source": [
        "Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72737oog2F6U"
      },
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_JUmf3G3b9S"
      },
      "source": [
        "show_random_elements(common_voice_train.remove_columns([\"path\", \"audio\"]), num_examples=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fowcOllGNNju"
      },
      "source": [
        "Alright! The transcriptions look fairly clean. Having translated the transcribed sentences, it seems that the language corresponds more to written-out text than noisy dialogue. This makes sense considering that [Common Voice](https://huggingface.co/datasets/common_voice) is a crowd-sourced read speech corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7OR50LN49m"
      },
      "source": [
        "We can see that the transcriptions contain some special characters, such as `,.?!;:`. Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. *E.g.*, the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\n",
        "Also in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.\n",
        "\n",
        "Let's simply remove all characters that don't contribute to the meaning of a word and cannot really be represented by an acoustic sound and normalize the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svKzVJ_hQGK6"
      },
      "source": [
        "import re\n",
        "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\â€œ\\%\\â€˜\\â€\\ï¿½\\']'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIHocAuTQbBR"
      },
      "source": [
        "common_voice_train = common_voice_train.map(remove_special_characters)\n",
        "common_voice_test = common_voice_test.map(remove_special_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxnVS9gIhIma"
      },
      "source": [
        "Let's look at the processed text labels again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBDRAAYxRE6n"
      },
      "source": [
        "show_random_elements(common_voice_train.remove_columns([\"path\",\"audio\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwfaptH5RJwA"
      },
      "source": [
        "Good! This looks better. We have removed most special characters from transcriptions and normalized them to lower-case only.\n",
        "\n",
        "Before finalizing the pre-processing, it is always advantageous to consult a native speaker of the target language to see whether the text can be further simplified.\n",
        "For this blog post, [Merve](https://twitter.com/mervenoyann) was kind enough to take a quick look and noted that \"hatted\" characters - like `Ã¢` - aren't really used anymore in Turkish and can be replaced by their \"un-hatted\" equivalent, *e.g.* `a`.\n",
        "\n",
        "This means that we should replace a sentence like `\"yargÄ± sistemi hÃ¢lÃ¢ saÄŸlÄ±ksÄ±z\"` to `\"yargÄ± sistemi hala saÄŸlÄ±ksÄ±z\"`.\n",
        "\n",
        "Let's write another short mapping function to further simplify the text labels. Remember - the simler the text labels, the easier it is for the model to learn to predict those labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZcrz6z7lgGm"
      },
      "source": [
        "def replace_hatted_characters(batch):\n",
        "    batch[\"trans\"] = re.sub('[Ã¢]', 'a', batch[\"sentence\"])\n",
        "    batch[\"sentence\"] = re.sub('[Ã®]', 'i', batch[\"sentence\"])\n",
        "    batch[\"sentence\"] = re.sub('[Ã´]', 'o', batch[\"sentence\"])\n",
        "    batch[\"sentence\"] = re.sub('[Ã»]', 'u', batch[\"sentence\"])\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieGhhND5mSwI"
      },
      "source": [
        "common_voice_train = common_voice_train.map(replace_hatted_characters)\n",
        "common_voice_test = common_voice_test.map(replace_hatted_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ORHDb2Th2TW"
      },
      "source": [
        "In CTC, it is common to classify speech chunks into letters, so we will do the same here.\n",
        "Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n",
        "\n",
        "We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars.\n",
        "It is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwCshNbbeRZR"
      },
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"sentence\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m6uUjjcfbjH"
      },
      "source": [
        "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
        "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oVgE8RZSJNP"
      },
      "source": [
        "Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQfneNsmlJI0"
      },
      "source": [
        "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0kRndSvqaKk"
      },
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
        "vocab_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOSzbvs9SXT1"
      },
      "source": [
        "Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) and we also extracted the special characters `\"\"` and `'`. Note that we did not exclude those special characters because:\n",
        "\n",
        "The model has to learn to predict when a word is finished or else the model prediction would always be a sequence of chars which would make it impossible to separate words from each other.\n",
        "\n",
        "One should always keep in mind that pre-processing is a very important step before training your model. E.g., we don't want our model to differentiate between `a` and `A` just because we forgot to normalize the data. The difference between `a` and `A` does not depend on the \"sound\" of the letter at all, but more on grammatical rules - *e.g.* use a capitalized letter at the beginning of the sentence. So it is sensible to remove the difference between capitalized and non-capitalized letters so that the model has an easier time learning to transcribe speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1fBRCn-TRaO"
      },
      "source": [
        "To make it clearer that `\" \"` has its own token class, we give it a more visible character `|`. In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Common Voice's training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npbIbBoLgaFX"
      },
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9yCWg4Sd0cb"
      },
      "source": [
        "Finally, we also add a padding token that corresponds to CTC's \"*blank token*\". The \"blank token\" is a core component of the CTC algorithm. For more information, please take a look at the \"Alignment\" section [here](https://distill.pub/2017/ctc/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znF0bNunsjbl"
      },
      "source": [
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFPGfet8U5sL"
      },
      "source": [
        "Cool, now our vocabulary is complete and consists of 37 tokens, which means that the linear layer that we will add on top of the pretrained MMS checkpoint as part of the adapter weights will have an output dimension of 37."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since a single MMS checkpoint can provide customized weights for multiple languages, the tokenizer can also consist of multiple vocabularies. Therefore, we need nest our `vocab_dict` to potentially add more languages to the vocabulary in the future. The dictionary should be nested with the name that is used for the adapter weights and that is saved in the tokenizer config under the name [`target_lang`](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.target_lang).\n",
        "\n",
        "Let's use the ISO-639-3 language codes as is used for the original [**`mms-1b-all`**](https://huggingface.co/facebook/mms-1b-all) checkpoint."
      ],
      "metadata": {
        "id": "Z1UUBPixLjqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_lang = \"tur\""
      ],
      "metadata": {
        "id": "T5Tr81luMPfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define an empty dictionary to which we can append the just created vocabulary"
      ],
      "metadata": {
        "id": "pNY3xJF1f74n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_dict = {target_lang: vocab_dict}"
      ],
      "metadata": {
        "id": "S5XNp1T2gENV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: In case you want to use this notebook to add a new adapter layer to *an existing model repo* use make sure to **not** create an empty, new vocab dict, but instead re-use one that already exists. To do so you should uncomment the following cells and replace `mms_adapter_repo` with a model repo id to which you want to add your adapter weights."
      ],
      "metadata": {
        "id": "uwEYeB4dfCKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "# mms_adapter_repo = \"Crynl/wav2vec2-large-mms-1b-nepali-colab\"  # make sure to replace this path with a repo to which you want to add your new adapter weights\n",
        "\n",
        "# tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(mms_adapter_repo)\n",
        "# new_vocab = tokenizer.vocab\n",
        "\n",
        "# new_vocab[target_lang] = vocab_dict"
      ],
      "metadata": {
        "id": "lGgSQMprfWn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CujRgBNVRaD"
      },
      "source": [
        "Let's now save the vocabulary as a json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehyUoh9vk191"
      },
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(new_vocab_dict, vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJDaKlIVVim"
      },
      "source": [
        "In a final step, we use the json file to load the vocabulary into an instance of the `Wav2Vec2CTCTokenizer` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xriFGEWQkO4M"
      },
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", target_lang=target_lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvL12DrNV4cx"
      },
      "source": [
        "If one wants to re-use the just created tokenizer with the fine-tuned model of this notebook, it is strongly advised to upload the `tokenizer` to the [ðŸ¤— Hub](https://huggingface.co/). Let's call the repo to which we will upload the files\n",
        "`\"wav2vec2-large-xlsr-turkish-demo-colab\"`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1XApZBAF2zr"
      },
      "source": [
        "repo_name = \"wav2vec2-large-mms-1b-nepali-colab\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1BiezWZF16d"
      },
      "source": [
        "and upload the tokenizer to the [ðŸ¤— Hub](https://huggingface.co/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytE1175GAKM"
      },
      "source": [
        "tokenizer.push_to_hub(repo_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwQM8lH_GGuP"
      },
      "source": [
        "Great, you can see the just created repository under `https://huggingface.co/<your-username>/wav2vec2-large-mms-1b-tr-colab`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYcIiR2FQ96i"
      },
      "source": [
        "### Create `Wav2Vec2FeatureExtractor`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6mDEyW719rx"
      },
      "source": [
        "Speech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called **sampling**. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the *real* speech signal but also necessitates more values per second.\n",
        "\n",
        "A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in data points being twice as long. Thus,\n",
        "before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n",
        "\n",
        "MMS was pre-trained at a sampling rate of 16kHz. Common Voice, in its original form, has a sampling rate of 48kHz, thus we will have to downsample the fine-tuning data to 16kHz in the following.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUbPW7oV-B5"
      },
      "source": [
        "A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:\n",
        "\n",
        "- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n",
        "- `sampling_rate`: The sampling rate at which the model is trained on.\n",
        "- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n",
        "- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n",
        "- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLS-R models checkpoints should **always** use the `attention_mask`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAR0-2KLkopp"
      },
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUETetgqYC3W"
      },
      "source": [
        "Great, MMS's feature extraction pipeline is thereby fully defined!\n",
        "\n",
        "For improved user-friendliness, the feature extractor and tokenizer are *wrapped* into a single `Wav2Vec2Processor` class so that one only needs a `model` and `processor` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYZtoW-tlZgl"
      },
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrKnYuvDIoOO"
      },
      "source": [
        "Next, we can prepare the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFmShnl7RE35"
      },
      "source": [
        "### Preprocess Data\n",
        "\n",
        "So far, we have not looked at the actual values of the speech signal but just the transcription. In addition to `sentence`, our datasets include two more column names `path` and `audio`. `path` states the absolute path of the audio file. Let's take a look.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCS7W6XJ9BG"
      },
      "source": [
        "common_voice_train[0][\"path\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6ndIjHGFp0W"
      },
      "source": [
        "MMS expects the input in the format of a 1-dimensional array of 16 kHz. This means that the audio file has to be loaded and resampled.\n",
        "\n",
        " Thankfully, `datasets` does this automatically by calling the other column `audio`. Let try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj_z5Zc3GAs9"
      },
      "source": [
        "common_voice_train[0][\"audio\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUUTgI1bGHW-"
      },
      "source": [
        "In the example above we can see that the audio data is loaded with a sampling rate of 48kHz whereas 16kHz are expected by the model. We can set the audio feature to the correct sampling rate by making use of [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrv65aj7G95i"
      },
      "source": [
        "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcnO4x-NGBEi"
      },
      "source": [
        "Let's take a look at `\"audio\"` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKtkc1o_HWHC"
      },
      "source": [
        "common_voice_train[0][\"audio\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOckzFd4Mbzq"
      },
      "source": [
        "This seemed to have worked! Let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded.\n",
        "\n",
        "**Note**: *You can click the following cell a couple of times to listen to different speech samples.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dueM6U7Ev0OA"
      },
      "source": [
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "rand_int = random.randint(0, len(common_voice_train)-1)\n",
        "\n",
        "print(common_voice_train[rand_int][\"sentence\"])\n",
        "ipd.Audio(data=common_voice_train[rand_int][\"audio\"][\"array\"], autoplay=True, rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY8m3vARHYTa"
      },
      "source": [
        "It seems like the data is now correctly loaded and resampled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaL9J2dNVtG"
      },
      "source": [
        "It can be heard, that the speakers change along with their speaking rate, accent, and background environment, etc. Overall, the recordings sound acceptably clear though, which is to be expected from a crowd-sourced read speech corpus.\n",
        "\n",
        "Let's do a final check that the data is correctly prepared, by printing the shape of the speech input, its transcription, and the corresponding sampling rate.\n",
        "\n",
        "**Note**: *You can click the following cell a couple of times to verify multiple samples.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Po2g7YPuRTx"
      },
      "source": [
        "rand_int = random.randint(0, len(common_voice_train)-1)\n",
        "\n",
        "print(\"Target text:\", common_voice_train[rand_int][\"sentence\"])\n",
        "print(\"Input array shape:\", common_voice_train[rand_int][\"audio\"][\"array\"].shape)\n",
        "print(\"Sampling rate:\", common_voice_train[rand_int][\"audio\"][\"sampling_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9teZcSwOBJ4"
      },
      "source": [
        "Good! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corresponds to 16kHz, and the target text is normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Pbn5WvOYZF"
      },
      "source": [
        "Finally, we can leverage `Wav2Vec2Processor` to process the data to the format expected by `Wav2Vec2ForCTC` for training. To do so let's make use of Dataset's [`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) function.\n",
        "\n",
        "First, we load and resample the audio data, simply by calling `batch[\"audio\"]`.\n",
        "Second, we extract the `input_values` from the loaded audio file. In our case, the `Wav2Vec2Processor` only normalizes the data. For other speech models, however, this step can include more complex feature extraction, such as [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).\n",
        "Third, we encode the transcriptions to label ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJY7I0XAwe9p"
      },
      "source": [
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
        "\n",
        "    batch[\"labels\"] = processor(text=batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Pg_WR3OGAP"
      },
      "source": [
        "Let's apply the data preparation function to all examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-np9xYK-wl8q"
      },
      "source": [
        "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
        "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcEWHvKI1by"
      },
      "source": [
        "**Note**: `datasets` automatically takes care of audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZWDCCKqwcfS"
      },
      "source": [
        "Awesome, now we are ready to start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlQkKVoRUos"
      },
      "source": [
        "## Training\n",
        "\n",
        "The data is processed so that we are ready to start setting up the training pipeline. We will make use of ðŸ¤—'s [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:\n",
        "\n",
        "- Define a data collator. In contrast to most NLP models, MMS has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning MMS requires a special padding data collator, which we will define below\n",
        "\n",
        "- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n",
        "\n",
        "- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration.\n",
        "\n",
        "After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slk403unUS91"
      },
      "source": [
        "### Set-up Trainer\n",
        "\n",
        "Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/7e61d56a45c19284cfda0cee8995fb552f6b1f4e/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py#L219).\n",
        "\n",
        "Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of MMS processor's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
        "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        labels_batch = self.processor.pad(\n",
        "            labels=label_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the\n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "source": [
        "from evaluate import load\n",
        "\n",
        "wer_metric = load(\"wer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qZU5p-deqB"
      },
      "source": [
        "The model will return a sequence of logit vectors:\n",
        "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
        "\n",
        "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmgrx4bRwLIH"
      },
      "source": [
        "Now, we can load the pretrained checkpoint of [**`mms-all-1b`**](https://huggingface.co/facebook/mms-1b-all). The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* ${}^2$.\n",
        "\n",
        "Since, we're only training a small subset of weights, the model is not prone to overfitting. Therefore, we make sure to disable all dropout layers.\n",
        "\n",
        "**Note**: When using this notebook to train MMS on another language of Common Voice those hyper-parameter settings might not work very well. Feel free to adapt those depending on your use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/mms-1b-all\",\n",
        "    attention_dropout=0.0,\n",
        "    hidden_dropout=0.0,\n",
        "    feat_proj_dropout=0.0,\n",
        "    layerdrop=0.0,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer),\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DwR3XLSzGDD"
      },
      "source": [
        "**Note**: It is expected that some weights are newly initialized. Those weights correspond to the newly initilaized vocabulary output layer.\n",
        "\n",
        "We now want to make sure that only the adapter weights will be trained and that the rest of the model stays frozen.\n",
        "\n",
        "First, we re-initialize all the adapter weights which can be done with the handy `init_adapter_layers` method. It is also possible to not re-initilize the adapter weights and continue fine-tuning, but in this case one should make sure to load fitting adapter weights via the [`load_adapter(...)` method](https://huggingface.co/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.load_adapter) before training. Often the vocabulary still will not match the custom training data very well though, so it's usually easier to just re-initialize all adapter layers so that they can be easily fine-tuned."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.init_adapter_layers()"
      ],
      "metadata": {
        "id": "3LcbD4PGP6CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we freeze all weights, **but** the adapter layers."
      ],
      "metadata": {
        "id": "xl8Y-njRQgrs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGI8zObtZ3V0"
      },
      "source": [
        "model.freeze_base_model()\n",
        "\n",
        "adapter_weights = model._get_adapters()\n",
        "for param in adapter_weights.values():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4aGhQM0K-D"
      },
      "source": [
        "In a final step, we define all parameters related to training.\n",
        "To give more explanation on some of the parameters:\n",
        "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
        "- `learning_rate` was chosen to be 1e-3 which is a common default value for training with Adam. Other learning rates might work equally well.\n",
        "\n",
        "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
        "\n",
        " To save GPU memory, we enable PyTorch's [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \"*mean*\".\n",
        "\n",
        " MMS adapter fine-tuning converges extremely fast to very good performance, so even for a dataset as small as 4h we will only train for 4 epochs.\n",
        "\n",
        "During training, a checkpoint will be uploaded asynchronously to the hub every 200 training steps. It allows you to also play around with the demo widget even while your model is still training.\n",
        "\n",
        "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=32,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=4,\n",
        "  gradient_checkpointing=True,\n",
        "  fp16=True,\n",
        "  save_steps=200,\n",
        "  eval_steps=100,\n",
        "  logging_steps=100,\n",
        "  learning_rate=1e-3,\n",
        "  warmup_steps=100,\n",
        "  save_total_limit=2,\n",
        "  push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Yg0mnd1lR7SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY7vBmFCPFgC"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=common_voice_train,\n",
        "    eval_dataset=common_voice_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXBx1JAA0DX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.\n",
        "\n",
        "${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3oKSzZ1hGq"
      },
      "source": [
        "Training should take less than 30 minutes depending on the GPU allocated to this notebook.\n",
        "\n",
        "In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (*right mouse click -> inspect -> Console tab and insert code*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYYAvgkW4P0m"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bGgLV2r0yvZ"
      },
      "source": [
        "Cool, let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fRr9TG5pGBl"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9q4mgMZplr_"
      },
      "source": [
        "The training loss and validation WER go down nicely.\n",
        "\n",
        "We see that fine-tuning adapter layers of `mms-1b-all` for just 100 steps outperforms fine-tuning the whole `xls-r-300m` checkpoint as shown [here](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2#training-1) already by a large margin.\n",
        "\n",
        "From the [official paper](https://scontent-cdg4-3.xx.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=fSo3qQ7uxr0AX8EWnWl&_nc_ht=scontent-cdg4-3.xx&oh=00_AfBL34K0MAAPb0CgnthjbHfiB6pSnnwbn5esj9DZVPvyoA&oe=6495E802) and this quick comparison it becomes clear that `mms-1b-all` has a much higher capability of transfering knowledge to a low-resource language and should be preferred over `xls-r-300m`. In addition, training is also more memory-efficient as only a small subset of layers are trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ya7WEy0pd13"
      },
      "source": [
        "The adapter weights will be uploaded as part of the model checkpoint, but we also want to make sure to save them seperately so that they can easily be off- and onloaded.\n",
        "\n",
        "Let's save all the adapter layers into the training output dir so that it can be correctly uploaded to the Hub."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import save_file as safe_save_file\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE\n",
        "import os\n",
        "\n",
        "adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)\n",
        "adapter_file = os.path.join(training_args.output_dir, adapter_file)\n",
        "\n",
        "safe_save_file(model._get_adapters(), adapter_file, metadata={\"format\": \"pt\"})"
      ],
      "metadata": {
        "id": "cMI_Q7DGdIgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can upload the result of the training to the ðŸ¤— Hub."
      ],
      "metadata": {
        "id": "uRgIquiPdkKv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArG1Thf6NBWm"
      },
      "source": [
        "trainer.push_to_hub()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main advantage of adapter weights training is that the \"base\" model which makes up roughly 99% of the model weights is kept unchanged and only a small [2.5M adapter checkpoint](https://huggingface.co/patrickvonplaten/wav2vec2-large-mms-1b-turkish-colab/blob/main/adapter.tur.safetensors) has to be shared in order to use the trained checkpoint.\n",
        "\n",
        "This makes it extremely simply to train additional adapter layers and add them to your repository.\n",
        "\n",
        "You can do some very easily by simply re-running this script and changing the language you would like to train on to a different one, *e.g.* `swe` for Swedish. In addition, you should make sure that the vocabulary does not get completely overwritten but that the new language vocabulary is **appended** to the existing one as stated above in the commented out cells.\n",
        "\n",
        "To demonstrate how different adapter layers can be loaded, I have trained and uploaded also an adapter layer for Swedish under the iso language code `swe` as you can see [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-mms-1b-turkish-colab/blob/main/adapter.swe.safetensors)"
      ],
      "metadata": {
        "id": "zh1j3PdQeEN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can load the fine-tuned checkpoint as usual by using `from_pretrained(...)`, but you should make sure to also add a `target_lang=\"<your-lang-code>\"` to the method so that the correct adapter is loaded. Also should you set the target language correctly for your tokenizer.\n",
        "\n",
        "Let's see how we can load the Turkish checkpoint first."
      ],
      "metadata": {
        "id": "K397I_Xsd594"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R351I9IQp_9D"
      },
      "source": [
        "model_id = \"Crynl/wav2vec2-large-mms-1b-nepali-colab\"\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=\"tur\").to(\"cuda\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "\n",
        "processor.tokenizer.set_target_lang(\"tur\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that the model can correctly transcribe Turkish"
      ],
      "metadata": {
        "id": "s_CQHW82kWHA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe2AE-2xqKHx"
      },
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice_test_tr = load_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", data_dir=\"./cv-corpus-6.1-2020-12-11\", split=\"test\", use_auth_token=True)\n",
        "common_voice_test_tr = common_voice_test_tr.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's process the audio, run a forward pass and predict the ids"
      ],
      "metadata": {
        "id": "qD2v7KT6kurE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pax07TnL3WZn"
      },
      "source": [
        "input_dict = processor(common_voice_test_tr[0][\"audio\"][\"array\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
        "\n",
        "pred_ids = torch.argmax(logits, dim=-1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epu8kCQZ3h70"
      },
      "source": [
        "\n",
        "Finally, we can decode the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4xWqmk_qMn0"
      },
      "source": [
        "print(\"Prediction:\")\n",
        "print(processor.decode(pred_ids))\n",
        "\n",
        "print(\"\\nReference:\")\n",
        "print(common_voice_test_tr[0][\"sentence\"].lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwhyoMml3oOT"
      },
      "source": [
        "This looks like it's almost exactly right, just two empty spaces should have been added in the first word."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is very simple to change the adapter to Swedish by calling [`model.load_adapter(...)`](mozilla-foundation/common_voice_6_1) and by changing the tokenizer to Swedish as well."
      ],
      "metadata": {
        "id": "2RhVgZEZlATe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_adapter(\"swe\")\n",
        "\n",
        "processor.tokenizer.set_target_lang(\"swe\")"
      ],
      "metadata": {
        "id": "gbSXNIKNmkVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We again load the Swedish test set from common voice"
      ],
      "metadata": {
        "id": "4_ysA-3tllPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_test_swe = load_dataset(\"mozilla-foundation/common_voice_6_1\", \"sv-SE\", data_dir=\"./cv-corpus-6.1-2020-12-11\", split=\"test\", use_auth_token=True)\n",
        "common_voice_test_swe = common_voice_test_swe.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "metadata": {
        "id": "2cthF_RQobAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " and transcribe a sample:"
      ],
      "metadata": {
        "id": "mRfCgOy6o6V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dict = processor(common_voice_test_swe[0][\"audio\"][\"array\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
        "\n",
        "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(processor.decode(pred_ids))\n",
        "\n",
        "print(\"\\nReference:\")\n",
        "print(common_voice_test_swe[0][\"sentence\"].lower())"
      ],
      "metadata": {
        "id": "5-bhaW_jotLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, this looks like a perfect transcription!\n",
        "\n",
        "We've shown in this blog post how MMS Adapter Weights fine-tuning not only gives state-of-the-art performance on low-resource languages, but also significantly speeds up training time and allows to easily build a collection of customized adapter weights.\n",
        "\n",
        "*Related posts and additional links are listed here:*\n",
        "- [**Official paper**](https://huggingface.co/papers/2305.13516)\n",
        "- [**Original cobebase**](https://github.com/facebookresearch/fairseq/tree/main/examples/mms/asr)\n",
        "- [**Official demo**](https://huggingface.co/spaces/facebook/MMS)\n",
        "- [**Transformers Docs**](https://huggingface.co/docs/transformers/index)\n",
        "- [**Related XLS-R blog post**](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)\n",
        "- [**Models on the Hub**](https://huggingface.co/models?other=mms)"
      ],
      "metadata": {
        "id": "HABoBg95sfYa"
      }
    }
  ]
}